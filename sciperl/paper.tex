\documentclass{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage[dvipsnames]{xcolor}
\usepackage[english]{babel}
\hypersetup{
     colorlinks=true,
     allcolors=NavyBlue
}
\usepackage{listings}
\lstset{language=Perl}

\begin{filecontents}{paper.bib}
    @article{oring,
        title = "The O-Ring Theory of Economic Development",
        author = "Michael Kremer",
        journal = "The Quarterly Journal of Economics, Vol. 108, No. 3",
        url     = {https://www.jstor.org/stable/2118400},
        year    = 1993,
        note    = "I say 'most' here, because there is a not insignificant sector of the economy called the 'foolproof' sector, see Garret Jones' 2012 paper 'The O-Ring Sector and the Foolproof Sector'.  However for the purposes of software, we can easily conclude much if not all of our industry is decidedly in the 'O-Ring' sector."
    }
    @misc{bezier,
        title  = "Software testing techniques",
        author = "Boris Bezier",
        note    = "Chapter 1, Section 1.7 - ISBN:0442206720",
        year    = 1990
    }
    @misc{prodprop,
        note = "While this was evident to even the early Taylorists, the idea did not become widely popular before Japanese adoption of Deming and Juran's principles."
    }
    @misc{bezier2,
        title  = "Bezier's software testing techniques",
        note   = "ibid"
    }
    @misc{bezier3,
        title  = "ibid",
        note   = "Chapter 2, Section 3.4.1"
    }
    @misc{turing,
        title  = "On Computable Numbers, with an Application to the Entscheidungsproblem",
        url    = {http://www.turingarchive.org/browse.php/B/12},
        author = "Alan Turing",
        year   = 1937
    }
    @misc{frame,
        title = "The Frame Problem",
        url = {https://plato.stanford.edu/entries/frame-problem/},
        note = "The current thinking as regards the resolution of the frame problem vis. AI is that intelligence, insofar as it exists at all, is 'embodied'.  Which is to say it has physical presence in the world, and intelligence is drawn forth from application of the body's faculties thereupon.  This explains a number of sociopolitical phenomenon such as the 'Flynn effect'."
    }
    @article{chapman,
        title = "Planning for conjunctive goals",
        journal = "Artificial Intelligence, Volume 32, Issue 3 pp. 333-377",
        url = {https://www.sciencedirect.com/science/article/abs/pii/0004370287900920},
        year = 1987,
        author = "David Chapman",
        note = "You can read it for free in MIT AI Technical Report 802 from 1985. Chapman proves NP-Completeness of the frame problem therein."
    }
    @misc{mises1,
        title = "Human Action",
        author = "Ludwig Von Mises",
        url    = {https://mises.org/library/book/human-action},
        year   = 1949,
        note   = "Ch. I, Sec IV"
    }
    @misc{mises2,
        title = "ibid",
        note  = "Ch. XXVI"
    }
    @misc{atwood,
        title  = "The God Login",
        author = "Jeff Atwood",
        url    = {https://blog.codinghorror.com/the-god-login/},
        year   = 2015,
        note   = "While I've heard many testers describe such comprehensive user surveillance systems they've built over the years, I've never heard them classified.  Perhaps the appropriate name might be 'revelation' tests."
    }
    @misc{schlep,
        title  = "Schlep Blindness",
        author = "Paul Graham",
        url    = {https://paulgraham.com/schlep.html},
        year   = 2012
    }
    @misc{gsb,
        url  = {https://github.com/teodesian/Pod-Coverage-More/blob/master/lib/Pod/Coverage/More.pm}
    }
    @misc{mltype,
        author = "Harry Mairson",
        year   = 1989,
        journal = "Proceedings of the ACM",
        url     = {https://dl.acm.org/doi/10.1145/96709.96748}
    }
    @misc{clippert,
        title  = "Lambda Expressions vs. Anonymous Methods, pt. 5",
        url    = {https://learn.microsoft.com/en-us/archive/blogs/ericlippert/lambda-expressions-vs-anonymous-methods-part-five},
        author = "Eric Clippert",
        year   = 2007
    }
    @misc{gsb2,
        note = "I once built a test script for just this, and then used the linux kernel testing api to put the zombies in uninterruptible sleep().  This is a fun way to DOS hypervisors."
    }
    @misc{mock,
        url = {https://metacpan.org/pod/Test::MockFile#How-this-mocking-is-done:}
    }
    @article{copilot,
        title="Measuring Github Copilot's Impact on Productivity",
        journal="Communications of the ACM",
        year=2024,
        author="Albert Ziegler",
        author="Eirini Kalliamvakou",
        author="X. Alice Li",
        author="Andrew Rice",
        author="Devon Rifkin",
        author="Shawn Simister",
        author="Ganesh Sittampalam",
        author="Edward Aftandilian",
        url={https://cacm.acm.org/research/measuring-github-copilots-impact-on-productivity/},
        note="'For context, in our study GitHub Copilot has an acceptance rate of 27\%'. While I've worked with some bad programmers (matthew 7:3), I've never worked long with ones who required 4 rounds of review to even be close to correct."
    }
    @book{standish,
        author="Jim Jordan, Standish Group",
        year=2021,
        title="CHAOS report: Beyond Infinity",
        note="Like Brooks' 'Mythical Man Month' predicted, it has borne out that software failure rates remain stubbornly high."
    }
\end{filecontents}

\title{%
    Reasoning about the rigor of perl programs: \\
    \large gaps, limits to understanding, and means to ameliorate
}
\author{%
    George S. Baugh \\
    \large CEO, Troglodyne LLC \\
    \url{https://troglodyne.net}
}
\begin{document}
\maketitle

\section{ABSTRACT}

While some tools exist to reason about the correctness of a program and its fitness for particular purposes,
there remain a number of unexamined approaches which will likely prove fruitful to explore.

The productivity of most capital or labor is \textit{almost entirely} a function of error rate \cite{oring}.
Being able to quantify the conditions under which failures can be expected is thus of the utmost importance.

I will lay out what is currently available, how that fits into a framework for reasoning about programs
in a rigorous fashion, and identify places where our ability to reason could be improved via tooling.

\newpage
\section{INTRODUCTION}

In software testing there's a concept known as the \textit{pesticide paradox} \cite{bezier} which is shorthand for saying
"The bugs are always where you aren't looking".  Abscence of evidence is not evidence of abscence.

If we can straightforwardly identify where we are not looking, and cross-reference this with
a means of discerning which bits of code are the most critical we can then prioritize our annealing process.

\subsection{Fundamental Testing Principles and Problems}

To identify where we are not looking, we need a proper mental model of how computers work if we are to test them.
Thankfully, this is well established.
Programs are just stacks of instructions capable of adding and removing instructions to said stack.
Modern programs are usually multiple series of these stacks in parallel, usually referred to as pipelines.
Some of these instructions are thought of as code, some as data but it's all the same to the executor.

Any given instruction is going to operate like any other mathematical function;
this is to say that it will have a range of acceptable inputs and outputs.
Anything outside of these ranges results in either invalid or undefined behavior when composed in a stack of instructions.
Much like a child's game of telephone\footnote{Also known as chinese whispers, and other names}, the earlier in the stack this happens, typically the worse the outcome.
Professional software testers are quick to tell you that this is \textit{also} true of the software \textit{production process} itself \cite{prodprop}.

From this two things are clear:
\begin{itemize}
\item rigorous testing of any individual \textit{instruction} within a stack over it's expected domain of inputs, and a sampling of data outside of that is valuable \cite{bezier2}.
\item The interaction of an instruction with its immediate follower and predecessor must also be well understood; these multi-instruction sections can indeed be treated as though they are themselves more complicated instructions.
\end{itemize}
These are the concepts behind two of the three\footnote{the third being acceptance or fitness testing which is discussed later} approaches to testing programs (unit\footnote{also referred to component testing} and integration\footnote{frequently confused with acceptance and black or grey-box testing} testing).

It is tempting from here to consider programs as discrete stacks, which can be exhaustively proven to behave well, in a \textit{proof via induction}.
However the data you feed in is itself a stack of instructions with a necessarily unbounded input and output domain.
To make things worse, well designed programs \textit{tend to} move code into data \cite{bezier3}, so now the problem not just undecidable \cite{turing} \textit{but also} recursive.

\subsection{Fitness: a further complication}

Another vexatious problem is encountered when demonstrating fitness for a particular purpose.
Any approach other than actually engaging in said purpose immediately falls afoul of the "frame problem" \cite{frame}.
Ultimately nothing but an actual user can actually accept a product is fit for a particular purpose, because non-embodied intelligence can't do that \cite{chapman}.
As such, any approach other than recording user journeys and being able to play them back is shooting in the dark.

Economists call this concept "Demonstrated Preference" and "Subjective Value"\cite{mises1}\footnote{it is my hypothesis that the impossibility of centralized economic planning \cite{mises2} is largely the frame problem restated.}.
For our purposes, it is sufficient to say that while computers are quite good at dealing with cardinal\footnote{a member of a cardinal set is \textit{equal, greater or lesser} than another} concepts,
they are astonishingly poor at dealing with ordinal\footnote{a member of an ordinal set is \textit{ordered before or after} another; X is preferred to Y.  Duplicates may effect said ordering based on a \textit{marginal utility function}.} ones.
Anyone familiar with sorting algorithms, counting problems and Ramsey's\footnote{Also known as the clique problem. See here for my latest brush with this: https://troglodyne.net/posts/6ff13eb0-13de-11ec-84d9-8f4e0c69f986} problem will have figured this out the hard way.
In this case, not only are we dealing with a problem which would be at least NP-Hard had we perfect knowledge, we also must grapple with incomplete information.

We are left with nothing more than the "God Algorithm" (already know the answer). \cite{atwood}
Frequently this is how caching systems are described, but to some extent this is also true of user surveillance.
This is one of many reasons that tools enabling comprehensive user surveillance have proven quite popular\footnote{Mostly for the purpose of advertisement}.

Nevertheless, many firms fall victim to the conceit of planning with \textit{user stories}\footnote{An idealized conception of a user and how their motivations might propel their journey through a usage session} and acceptance tests based around these.
Ultimately the only \textit{effective} means to catch fitness issues early in new features is to \textit{eat your own dog food}\footnote{Origins of the phrase are largely lost to time, but the practice was generally accepted in American firms before the 1990s software boom.},
being particularly attentive to "Schleps" \cite{schlep} and other places the work-flow can be de-burred or otherwise streamlined\footnote{The most popular systematic approaches to this being Kaizen and Kanban}.

\section{Reasoning about Perl Programs}

Knowing all this, what then shall we do?
First, let's go over the extant tools on CPAN\footnote{Comprehensive Perl Archive network.  https://cpan.org}.

All programs can be expressed as lambda calculus; programs are nearly always transformed into some variety of "intermediate form" prior to being handed off to lower level executors.
One such form is the "abstract syntax tree", which is an intermediate form used to reason about the correctness of programs rather than to pass to hardware for execution.

Building such a tree is how PPI\footnote{An important distinction must be made here: the intermediate form is of Perl \textit{documents} rather than Perl \textit{programs}; which is to say an idealized conception of Perl which does not necessarily line up with interpreted behavior, but is close enough to be useful.},
and the tools which rely on it such as \texttt{Perl::Critic}\footnote{https://metacpan.org/pod/Perl::Critic} work.
Alternatively you can inspect the "intermediate form" which the interpreter itself builds via the \texttt{B::Deparse}\footnote{https://metacpan.org/pod/B::Deparse},
or similar techniques such as \texttt{Devel::Cover}\footnote{https://metacpan.org/pod/Devel::Cover} and \texttt{Devel::NYTProf}\footnote{https://metacpan.org/pod/Devel::NYTProf} leverage.
However the ability to reason about Perl programs in the same fashion as any other programming language does not guarantee our avenues of inquiry are exhaustively explored.

\subsection{Devel::Cover does not quite cover the subject}

\texttt{Devel::Cover} is quite good at identifying statements and conditions which are not covered within a program.
It cannot however identify places where a condition ought to be applied, but is not or vice versa.

It also does not have a means of identifying whether the boundaries of any given block's input/output domain have been exercised.
It is generally via testing these boundaries that needs for additional conditional statements\footnote{Sometimes referred to guard clauses, short-circuits, etc} are discovered.
Unfortunately the means available to automatically discover these boundaries is lacking.

Perl only has optional input signatures\footnote{https://perldoc.pl/perlsub\#Signatures}, and no output signatures.
Nor are there ways to signify that a subroutine throws exceptions.
All this must be inferred via some means.

This inference necessarily implies walking the stack down to the most basic of calls.
Suppose we have a sub foo, which calls bar() that calls baz():

\begin{lstlisting}

sub foo {
    bar(@_);
}

sub bar {
   my ($a, %b) = @_;
   baz($_[2]);
   ...
}

sub baz {
   close($_[0]) or die;
}

\end{lstlisting}

There is little we can say with confidence here.
What we can say (without seeing the remaining body of bar) is that:
\begin{itemize}
\item The second argument to foo() is a likely a hash due to assignment (but might not be, as it is then later called-by-reference!)
\item This hash is (hopefully) one key/value (as randomization will break this code due to aforementioned call-by-reference of the third argument which is presumably the hash value)
\item This value ought to be an \textit{open} filehandle, given it is then passed to close()
\item In the event it is not, the subroutine throws an exception.
\end{itemize}
As you can see, there is \textit{considerable} ambiguity in even quite trivial examples.

Some tool which automates this reasoning, likely a CPAN module (Which might appropriately be named \texttt{Sub::Characterizer}), would be of great utility.
It could be used alongside \texttt{Pod::Weaver}\footnote{https://metacpan.org/pod/Pod::Weaver} annotations to largely auto-generate subroutine documentation as well.
\texttt{Pod::Coverage}\footnote{https://metacpan.org/pod/Pod::Coverage} could be similarly enhanced.

The difficulty here is twofold: the scope of such an undertaking and the computational difficulty to do so.
A lookup table of every subroutine in Perl's CORE\footnote{\texttt{perldoc perlfunc}} namespace's arguments mapping to perl's internal datatypes \footnote{see \texttt{perldoc perlguts}} would need to be built.
The same would have to be done for every FFI\footnote{Foreign Function Interface} or XS\footnote{\texttt{perldoc perlxs}} module used.
Given all other Perl subroutines you write are going to be a composition thereof, this knowledge would allow for correct characterizations of subroutines to be made.

There are \textit{significant} costs involved in this, particularly for FFI/XS, which is much of CPAN.
This is because the FFI interface itself (be it XS, libffi\footnote{the backend for \texttt{FFI::Platypus}}, or others) would have to produce this information as a build artifact
which could then be used to instruct the characterizer appropriately.

In addition, a set of boundary data for each of these datatypes would need to be maintained going forward, along with whether fatal exceptions may occur.
Even then it would be of great utility to have some means of annotating when a subroutine's arguments acceptable input domain is a subset of its governing datatype.
I found most of this out the "hard way" \cite{gsb} long ago attempting to solve this particular problem (and failing).

Nevertheless, this is a problem which others have attempted to solve and done so with some success.
Haskell, ML and Swift all have such type inference systems with the caveat of exponential worst-case compile times\cite{clippert}.
This is because the problem is in fact the 3SAT problem\footnote{also known as the boolean satisfiability problem} which is NP-hard\cite{mltype}.

Like all other NP-hard problems, they can be overcome by solving a simpler subset of the problem (aka "cheating").
A type inferencer can be lifted from one of these languages, and if we can devise a means to studiously avoid the perverse cases it would be of great utility.

\subsection{Surely strong typing solves all this?}

Some users of other programming languages believe themselves to have obviated the entire class of errors due to improper inputs via strong typing systems.
This of course ignores the reality that poor choice of type (such as an "Any" type, or unsigned when you need signs, etc) immediately voids any such guarantees.
However the argument is not entirely without merit, and there are various runtime constraint systems such as \texttt{Type::Tiny} attesting to this\footnote{Along with other efforts, such as those promoted by the Perl Types committee: https://perlcommunity.org/types/}.

Bolting on strict types to a dynamic language is not a subject to approach lightly, as many features like late binding and reflection become far more difficult.
We should learn from existing successful attempts to do so.

Typescript\footnote{a ECMAScript (aka Javascript) code generator} is the most successful example of a dynamic language figuring out how to bolt-on static types.
Supposing everything else it interacts with is also generated by Typescript\footnote{Rarely ever the case in Javascript projects}, a good degree of confidence can be had.

\subsection{100\% coverage, or: how many angels can dance upon the head of a pin?}

It's usually fairly straightforward to get 100\% statement coverage on greenfield\footnote{ex nihilo} code.
However this rarely translates into software quality, as the test coverage for any of the dependencies in the stack is opaque in practically all programming languages.
How do we identify if the dependencies we use are well covered \textit{where we use them}?

First, we must run the testsuites of our dependencies when installing them.
This is (normally) the case when using something like perlbrew, with an isolated environment.
System perl is usually another story, as many modules will be shipped with another packaging manager.

While we could always wrap CPAN, ideally a CPAN::Plugin either pre or post test would be ideal.
Given the wide variety of means that CPAN Modules might be installed, any solution here would have to build a coverage index on-demand (likely caching it for later use).
cpancover \textit{appears} it \textit{intends to} do this, but is currently broken and appears unmaintained.

The same techniques can be cross-referenced with the relative importance of the module.
This information is known as the \textit{CPAN River}\footnote{See http://neilb.org/2015/04/20/river-of-cpan.html}.
Doing this will identify where the most high-impact test coverage improvements can be made in the perl ecosystem.

\subsection{eval \{\} is not enough}

Since not all subroutines are \textit{Klingon Programming}\footnote{where it is better to \textit{die()} than \textit{return()} in failure.  Also known as "Control-Flow by Exception".  Such subroutines usually behave poorly in threaded contexts.},
failures-by-value tend to go unnoticed when inured in this mental model.
This is why a great many Perl::Critic policies exist to catch precisely such problems caused by external factors largely outside of a subroutine's control.
For example, I have observed policies enforcing code along these lines:
\begin{lstlisting}
# Failing to have a 'do' clause after block eval without
# assignment generally means something is failing silently
eval {...;1;} or do {...}

# Failing to check that you got a defined pid from fork()
# means the program will have undefined behavior in the
# event the process table is full.
my $pid=fork() // die;
\end{lstlisting}
Some means of actual fault injection should also be taken into account in addition to standard coverage metrics as a measure of the code's overall robustness.

The sources of these faults outside of a subroutine's control are generally from syscalls or FFI\footnote{Foreign function interface}.
As such we will need some means of discerning whether or not the usage in a test behaves well under these scenarios:
\begin{itemize}
\item call emits (or is subjected to) a signal (e.g. SIGUSR1, SIGHUP, SIGKILL) either directly or in a child
\item call emits an exception
\item call returns a failure code
\item call appears successful, but nonetheless failed
\end{itemize}
Ideally, this would be done in a semi-automated fashion where one could write a test exercising the expected operational modes, and then re-execute subjected to permutations of these failures.
The approach used by \texttt{Test::MockFile}\cite{mock} is likely one which can be leveraged to inject syscall faults, and everything else can be handled with a universal mock (which would wrap whatever existing mock or sub is called.).

Automatically identifying all the subroutine calls within a given subroutine to inject these faults is complicated and relatively costly.
A look-ahead using a similar technique as used in \texttt{Devel::Trace}\footnote{https://metacpan.org/pod/Devel::Trace} (or internal interpreter knowledge) would need to be used to know whether we are in the right place.
Furthermore, we must pick the subroutines out of the next line if we are in the right place and wrap/mock them appropriately.

Thankfully there is a solution which uses XS to accomplish this: \texttt{Devel::Probe}\footnote{https://metacpan.org/pod/Devel::Probe}.
There is also a more fleshed-out and generic framework called \texttt{Devel::Events}\footnote{https://metacpan.org/pod/Devel::Events}.

\subsection{Fitness for a particular purpose}

As mentioned earlier, the best means to evaluate whether changes to software harm it's fitness to purpose is some means of tracking user paths through the software as they perform tasks.
If sudden unintended disturbances in user paths are detected after pushing changes, it is a strong indication of error.

This is generally done via coupling a powerful logging framework such as \texttt{Log::Dispatch::DBI}\footnote{https://metacpan.org/pod/Log::Dispatch::DBI} and robust session handling using unique identifiers.
From there you need something to \textit{identify and classify} common patterns of usage.
This sounds suspiciously like what Neural Networks are \textit{actually good at doing}.

\texttt{AI::Categorizer} running on a \texttt{pack()}ed representation of an application's state parameters throughout a user session ought to do the trick.

\subsection{Quality with non-functional Characteristics}

Tools for reasoning about the performance of perl programs such as \texttt{Devel::NYTProf}\footnote{https://metacpan.org/pod/Devel::NYTProf} cover the subject of performance well.
However, the ability to reason about the accessibility, localizability or portability of your program is crude at best.
Generally the only option here is to impose linter rules which force the developer into the practices necessary to facilitate these things.
The only systematic attempt to apply such linting across the perl ecosystem is CPANTS\footnote{https://cpants.cpanauthors.org/},
but this could stand to be extended with more categories of evaluation.

For example, suppose I have the option of multiple modules on CPAN which I see do roughly the same thing in function X.
It would greatly simplify my decision as to what to use if I already knew ahead of time:
\begin{itemize}
\item how does it perform?
\item How complex is it?  I may have to read and debug this.
\item Is it doing non-portable things?
\end{itemize}
And so on.

This is also another place where AI techniques could be of great utility.
While they are necessarily inept at writing \textit{correct} code \cite{copilot}, they're pretty great at classifying code.
As such we could likely build these lists of "Modules that do largely the same things on CPAN" in a mostly automated fashion.
Currently, the best we have is the "SEE ALSO" section of a module's POD, which is far from comprehensive.

Coupling CPANTS to a classifier would make it far more useful, stimulating the development of these additional checks which do not currently exist.
Being able to sort a list of candidate modules for a particular purpose by \texttt{kwalitee}\footnote{CPANTS refers to non-functional parameters as kwalitee}
is one way to accomplish that.

\newpage
\section{Conclusion}

As you might expect from an active programming community of 35+ years, the things Perl programmers struggle to understand about their programs are what every programming language community struggles with.
The good news is that our incapacity to percieve all the ways in which our programs can fail also means automated solutions to ferreting these out are likely to be far more effective at doing so than users.
Much of this tooling has yet to have been built, and recent advances have put a number of them tantalizingly within our grasp.

The software industry is one of the few places aside from SoEs\footnote{State-Owned Enterprises} where a 2/3 rate of failure \cite{standish} is remotely tolerable.
This is because our field is still relatively young (and thus can afford such waste due to high profit margins) but that is changing quickly.
Programming communities that do not improve their tooling to enable writing programs with less defects will over time be out-competed by those that do.

\newpage
\bibliographystyle{vancouver}
\bibliography{paper}

\end{document}
