\documentclass{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage[dvipsnames]{xcolor}
\hypersetup{
     colorlinks=true,
     allcolors=NavyBlue
}
\usepackage{listings}
\lstset{language=Perl}

\begin{filecontents}{paper.bib}
    @misc{antithesis,
        title   = "Is something Bugging you?",
        author  = "Will Wilson",
        url     = {https://antithesis.com/blog/is_something_bugging_you/},
        year    = 2024
    }
\end{filecontents}

\title{%
    Reasoning about the rigor of perl programs: \\
    \large gaps, limits to understanding, and means to ameliorate
}
\author{%
    George S. Baugh \\
    \large CEO, Troglodyne LLC \\
    \url{https://troglodyne.net}
}
\begin{document}
\maketitle

\section{ABSTRACT}

While some tools exist to reason about the correctness of a program and its fitness for particular purposes,
there remain a number of unexamined approaches which will likely prove fruitful to explore.

Productivity of any piece of capital or laborer is entirely a function of error rate; being able to quantify
the conditions under which failures can be expected is thus of the utmost importance.

In the era of testsuite generation via large language models, this is of even greater concern.

I will lay out what is currently available, how that fits into a framework for reasoning about programs
in a rigorous fashion, and identify places where our ability to reason could be improved via tooling.
Prototypes of said tools will also be demonstrated.

\newpage
\section{INTRODUCTION}

In software testing there's a concept known as the "pesticide paradox" which is shorthand for saying
"The bugs are always where you aren't looking".  Abscence of evidence is not evidence of abscence.

If we can straightforwardly identify where we are not looking, and cross-reference this with
a means of discerning which bits of code are the most critical we can then prioritize our annealing process.

To identify where we are not looking, we need a proper mental model of how computers work.
Thankfully, this is well established.  Programs are just stacks of instructions capable of adding and removing instructions to said stack.
Modern programs are usually multiple series of these stacks in parallel, usually referred to as pipelines.
Some of these instructions are thought of as code, some as data but it's all the same to the executor.

Any given instruction is going to operate like a traditional mathematical function;
this is to say that it will have a range of acceptable inputs and outputs.
Anything outside of these ranges results in either invalid or undefined behavior when composed in a stack of instructions.
Much like a child's game of "telephone", the earlier in the stack this happens, typically the worse the outcome.

From this two things are clear:
\begin{itemize}
\item rigorous testing of any individual \textit{instruction} within a stack over it's expected domain of inputs, and a sampling of data outside of that is valuable.
\item The interaction of an instruction with its immediate follower and predecessor must also be well understood; these multi-instruction sections can indeed be treated as though they are themselves more complicated instructions.
\end{itemize}
This is the actual theoretical underpinning behind the two testing concepts we are all familiar with (unit and integration testing), but most don't think too hard about.
It's this lack of concrete understanding that leads to frequent confusion and ineffective testsuites the world over.

It is tempting from here to consider programs as discrete stacks, which can be exhaustively proven to behave well, in a sort of "proof by induction".
However the data you feed in is itself a stack of instructions with a necessarily unbounded input and output domain.
This is best known as the "halting problem", as there is always theoretically some set of data you can feed a program which will prevent any program from ever terminating.
What is less well understood is that the pesticide paradox is pernicious precisely because it is the halting problem.
And the halting problem is actually Godel's Uncertainty principle - there will always exist at least one statement in any given axiomatic system which cannot ever be proven.

This same problem rears its head when attempting to demonstrate fitness for a particular purpose!
Any approach other than actually engaging in said purpose immediately falls afoul of the "frame problem".
Ultimately nothing but an actual user can actually accept a product is fit for a particular purpose.
As such, any approach other than recording user journeys and being able to play them back is shooting in the dark.

Economists call this concept "Demonstrated Preference", and the impossibility of centralized economic planning is largely the frame problem restated.
Many firms fall victim to this conceit with their "user stories" and acceptance tests based around these.
Ultimately the only means to catch fitness issues early in new features is to eat your own dog food, being particularly attentive to "Schlepping" and other places the work-flow can be de-burred.

\section{Reasoning about Perl Programs}

All programs can after all be expressed as lambda calculus; programs are nearly always transformed into some variety of "intermediate form" prior to being handed off to lower level executors.
One such form is the "abstract syntax tree", which is an intermediate form used to reason about the correctness of programs rather than to pass to hardware for execution.

Building such a tree is how PPI, and the tools which rely on it such as Perl::Critic work.
Alternatively you can inspect the "intermediate form" which the interpreter itself builds via the B::Deparse, or similar techniques as Devel::Cover and Devel::NYTProf leverage.
However just because you \textit{can} reason about perl programs in the same fashion as any other programming language, does not guarantee avenues of inquiry are exhaustively explored.

\subsection{Devel::Cover does not quite cover the subject}

Devel::Cover is quite good at identifying statements and conditions which are not covered within a program.
It cannot however identify places where a condition ought to be applied, but is not.

It also does not have a means of identifying whether the boundaries of any given block's input/output domain have been exercised.
It is generally via these means that we discover these missed conditionals, as we are frequently surprised by the realities imposed by input data.
Given perl only has optional input signatures, and no output signatures nor ways to signify that a sub throws, this has to be inferred; a characterization of the stack must be built.

This inference necessarily implies walking the stack down to the most basic of calls.
Suppose we have a sub foo, which calls bar() that calls baz():

\begin{lstlisting}

sub foo {
    bar(@_);
}

sub bar {
   my ($a, %b) = @_;
   baz($_[2]);
   ...
}

sub baz {
   close($_[0]) or die;
}

\end{lstlisting}

All we can say with confidence (without seeing the remaining body of bar) is that the second argument to foo() is a hash which is likely only one key/value (as randomization will break this code), the value of which ought to be a filehandle, and which will throw in the event it isn't.

Some tool which automates this reasoning, say a Sub::Characterizer would be of great utility. It could be used alongside Pod::Weaver annotations to largely auto-generate documentation as well.
The core difficulty here is the sheer scope of such an undertaking.  A lookup table of every call in core's arguments vis perlguts datatypes, along with every FFI interaction would have to be built, likely during BEGIN.
From there, a set of boundary data for each of these datatypes would need to be maintained going forward.

Some users of other programming languages believe themselves to have obviated the entire class of errors due to improper inputs via strong typing systems.
This of course ignores the reality that poor choice of type (such as an "Any" type, or unsigned when you need signs, etc) immediately voids any such guarantees, and boundary testing is nonetheless necessary.

\subsection{But I've got 100\% coverage, now what?}

It's usually fairly straightforward to get 100% statement coverage on greenfield code.
However this rarely translates into software quality, as the test coverage on any of the deps in the stack remains opaque.
How do we identify if the dependencies we use are well covered \textit{where we use them}?
If we can, the same techniques can be cross-referenced with the relative importance of the module vis. the "CPAN River" to identify where the most high-impact test coverage improvements could be made in the perl ecosystem.

It turns out that we can do this, supposing we actually run the testsuites of our dependencies when installing them.
This is (normally) the case when using something like perlbrew, with an isolated environment.
System perl is usually another story, as many modules will be shipped with another packaging manager.

While we could always wrap CPAN, ideally a CPAN::Plugin either pre or post test would be ideal.
Nevertheless, given the wide variety of means that CPAN Modules might be installed, any solution here would have to build a coverage index on-demand, and cache it for later use, likely as a plugin to Devel::Cover instead.
cpancover \textit{appears} to do this, but is currently broken and appears unmaintained.

\subsection{eval \{\} is not enough}

Since the primary programming paradigm is \textit{not} "Klingon Programming" (where it is better to \textit{die()} than \textit{return()} in failure), there is no guarantee that environmental problems are caught.
This is why a great many Perl::Critic rules about using eval {...;1;} or do..., `my $pid=fork(); die unless defined $pid` and so on exist.
Some means of actual fault injection should also be taken into account with regard to coverage.

How many subs are actually safe when `ls` returns `read error`?
Does your script recover from children whacked by oomkiller?
What if fork() fails because the process table is filled with zombies?
What about system() or qx{} or pipe open() when the target does not exist, but that -f returns true because you have a bad RAID card?

This is actually a quite complicated subject.  As the founder of Antithesis puts it \cite{antithesis}:
\begin{displayquote}
the \$100 bill is surrounded by an entire field of poisonous spike-pits, nay, a continent of them.
\end{displayquote}
Aside from building something which can inject the \textit{astonishingly vast array} of such faults, there is still the problem of integrating this into perl's testing harness, and reporting coverage to do.

\subsection{Fitness for a particular purpose}

As mentioned earlier, the best means to evaluate whether changes to software harm it's fitness to purpose is some means of tracking user paths through the software as they perform tasks.
There are two levels of this; one is front-end and the other on the back end.
Ideally these are integrated somehow; given perl is not *usually* the code powering UIs, but generating them, our primary concern will be with the points that interfaces reach out and touch backend code.

The best way to satisfy these needs is via a powerful logging framework using something like Log::Dispatch::DBI and robust session handling with shared identifiers between frontend and backend instrumentation.
From there you need something to *identify and classify* common patterns of usage.
This sounds suspiciously like what Neural Networks are *actually good at doing*, so let's do just that.

\subsection{Quality with non-functional Characteristics}

Devel::NYTProf is pretty much the best profiler in the industry; it's one of the primary reasons to recommend perl over alternatives.
However, the ability to reason about the accessibility, localizability or portability of your program is crude at best.
Generally the only option here is to impose linter rules which force the developer into the practices necessary to facilitate these things.
The only systematic attempt to apply linting across the perl ecosystem is CPANTS, but this could stand to be extended with more categories of evaluation.

For example, suppose I have the option of multiple modules on CPAN which I see do roughly the same thing in function X.
It would greatly simplify my decision as to what to use if I already knew ahead of time:
\begin{itemize}
\item What's the bandwidth of this function; how does it perform?
\item How complex is it?  I may have to read and debug this.
\item Is it doing non-portable things?
\end{itemize}
And so on.

This is also another place where NNs could be of great utility; while they are pretty awful at writing code, they're pretty great at classifying it.
As such we could likely build these lists of "Modules that do largely the same things on CPAN" in a mostly automated fashion.
You could roll right up to metacpan and see a "More like this" link -- like youtube but actually useful!
I for one would love seeing a table with these criteria and test coverage readily available -- no other packaging system does anything approaching this useful.

Oftentimes your choice of components is the primary determinant of your output's quality; we could do a lot more to help people reason about this.

\newpage
\section{Conclusion}

As you might expect from an active programming community of 35+ years, the things perl struggles to understand about its programs are essentially what every other programming language struggles with.
This should come as no shock, as the ultimate cause of them are pretty sticky fundamental issues on the "even people don't qualify as Strong AI" spectrum.
The good news is that our near-total incapacity to percieve all the ways in which our programs can fail also means automated solutions to ferreting these out are likely to be far more effective at doing so than users.
As such the dream of "Bug Free" software is largely attainable in the same sense that a house in a bad neighborhood with cameras on it is going to be largely burgular free.
Being of relatively higher quality than all your competitors is highly desirable, as quality is the *entire* driver of productivity and income.
You don't have to be original, use non-dead programming languages or even be particularly talented to win with this approach. (japan.jpg)

The software industry is one of the few places aside from SoEs (nasa.jpg) where this rate of failure is remotely tolerable.
This is because our field is still relatively young, but that is changing quickly.
The trend you need to get religion on is not AI, but quality.

\newpage
\bibliographystyle{vancouver}
\bibliography{paper}

\end{document}
