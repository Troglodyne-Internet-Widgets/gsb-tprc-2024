\documentclass{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage[dvipsnames]{xcolor}
\usepackage[english]{babel}
\hypersetup{
     colorlinks=true,
     allcolors=NavyBlue
}
\usepackage{listings}
\lstset{language=Perl}

\begin{filecontents}{paper.bib}
    @article{oring,
        title = "The O-Ring Theory of Economic Development",
        author = "Michael Kremer",
        journal = "The Quarterly Journal of Economics, Vol. 108, No. 3",
        url     = {https://www.jstor.org/stable/2118400},
        year    = 1993,
        note    = "I say 'most' here, because there is a not insignificant sector of the economy called the 'foolproof' sector, see Garret Jones' 2012 paper 'The O-Ring Sector and the Foolproof Sector'.  However for the purposes of software, we can easily conclude much if not all of our industry is decidedly in the 'O-Ring' sector."
    }
    @misc{bezier,
        title  = "Software testing techniques",
        author = "Boris Bezier",
        note    = "Chapter 1, Section 1.7 - ISBN:0442206720",
        year    = 1990
    }
    @misc{prodprop,
        note = "While this was evident to even the early Taylorists, the idea did not become widely popular before Japanese adoption of Deming and Juran's principles."
    }
    @misc{bezier2,
        title  = "Bezier's software testing techniques",
        note   = "TODO: chapter/verse"
    }
    @misc{bezier3,
        title  = "ibid",
        note   = "Chapter 2, Section 3.4.1"
    }
    @misc{turing,
        title  = "On Computable Numbers, with an Application to the Entscheidungsproblem",
        url    = {http://www.turingarchive.org/browse.php/B/12},
        author = "Alan Turing",
        year   = 1937
    }
    @misc{frame,
        title = "The Frame Problem",
        url = {https://plato.stanford.edu/entries/frame-problem/},
        note = "The current thinking as regards the resolution of the frame problem vis. AI is that intelligence, insofar as it exists at all, is 'embodied'.  Which is to say it has physical presence in the world, and intelligence is drawn forth from application of the body's faculties thereupon.  This explains a number of sociopolitical phenomenon such as the 'Flynn effect'."
    }
    @article{chapman,
        title = "Planning for conjunctive goals",
        journal = "Artificial Intelligence, Volume 32, Issue 3 pp. 333-377",
        url = {https://www.sciencedirect.com/science/article/abs/pii/0004370287900920},
        year = 1987,
        author = "David Chapman",
        note = "You can read it for free in MIT AI Technical Report 802 from 1985. Chapman proves NP-Completeness of the frame problem therein."
    }
    @misc{antithesis,
        title   = "Is something Bugging you?",
        author  = "Will Wilson",
        url     = {https://antithesis.com/blog/is_something_bugging_you/},
        year    = 2024,
        note    = "Having built similar systems myself, I second these sentiments."
    }
    @misc{mises1,
        title = "Human Action",
        author = "Ludwig Von Mises",
        url    = {https://mises.org/library/book/human-action},
        year   = 1949,
        note   = "Ch. I, Sec IV"
    }
    @misc{mises2,
        title = "ibid",
        note  = "Ch. XXVI"
    }
    @misc{atwood,
        title  = "The God Login",
        author = "Jeff Atwood",
        url    = {https://blog.codinghorror.com/the-god-login/},
        year   = 2015,
        note   = "While I've heard many testers describe such comprehensive user surveillance systems they've built over the years, I've never heard them classified.  Perhaps the appropriate name might be 'revelation' tests." 
    }
    @misc{schlep,
        title  = "Schlep Blindness",
        author = "Paul Graham",
        url    = {https://paulgraham.com/schlep.html},
        year   = 2012
    }
    @misc{gsb,
        url  = {https://github.com/teodesian/Pod-Coverage-More/blob/master/lib/Pod/Coverage/More.pm}
    }
    @misc{gsb2,
        note = "I once built a test script for just this, and then used the linux kernel testing api to put the zombies in uninterruptible sleep().  This is a fun way to DOS hypervisors."
    }
    @misc{mock,
        url = {https://metacpan.org/pod/Test::MockFile#How-this-mocking-is-done:}
    }
    @article{copilot,
        title="Measuring Github Copilot's Impact on Productivity",
        journal="Communications of the ACM",
        year=2024,
        author="Albert Ziegler",
        author="Eirini Kalliamvakou",
        author="X. Alice Li",
        author="Andrew Rice",
        author="Devon Rifkin",
        author="Shawn Simister",
        author="Ganesh Sittampalam",
        author="Edward Aftandilian",
        url={https://cacm.acm.org/research/measuring-github-copilots-impact-on-productivity/},
        note="'For context, in our study GitHub Copilot has an acceptance rate of 27\%'. While I've worked with some bad programmers (matthew 7:3), I've never worked long with ones who required 4 rounds of review to even be close to correct."
    }
    @book{standish,
        author="Jim Jordan, Standish Group",
        year=2021,
        title="CHAOS report: Beyond Infinity",
        note="Like Brooks' 'Mythical Man Month' predicted, it has borne out that software failure rates remain stubbornly high."
    }
\end{filecontents}

\title{%
    Reasoning about the rigor of perl programs: \\
    \large gaps, limits to understanding, and means to ameliorate
}
\author{%
    George S. Baugh \\
    \large CEO, Troglodyne LLC \\
    \url{https://troglodyne.net}
}
\begin{document}
\maketitle

\section{ABSTRACT}

While some tools exist to reason about the correctness of a program and its fitness for particular purposes,
there remain a number of unexamined approaches which will likely prove fruitful to explore.

The productivity of most capital or labor is \textit{almost entirely} a function of error rate \cite{oring}.
Being able to quantify the conditions under which failures can be expected is thus of the utmost importance.

In the era of testsuite generation via large language models, this is of even greater concern.

I will lay out what is currently available, how that fits into a framework for reasoning about programs
in a rigorous fashion, and identify places where our ability to reason could be improved via tooling.
Prototypes of said tools will also be demonstrated.

\newpage
\section{INTRODUCTION}

In software testing there's a concept known as the \textit{pesticide paradox} \cite{bezier} which is shorthand for saying
"The bugs are always where you aren't looking".  Abscence of evidence is not evidence of abscence.

If we can straightforwardly identify where we are not looking, and cross-reference this with
a means of discerning which bits of code are the most critical we can then prioritize our annealing process.

\subsection{Fundamental Testing Principles and Problems}

To identify where we are not looking, we need a proper mental model of how computers work if we are to test them.
Thankfully, this is well established.
Programs are just stacks of instructions capable of adding and removing instructions to said stack.
Modern programs are usually multiple series of these stacks in parallel, usually referred to as pipelines.
Some of these instructions are thought of as code, some as data but it's all the same to the executor.

Any given instruction is going to operate like any other mathematical function;
this is to say that it will have a range of acceptable inputs and outputs.
Anything outside of these ranges results in either invalid or undefined behavior when composed in a stack of instructions.
Much like a child's game of "telephone", the earlier in the stack this happens, typically the worse the outcome.
Professional software testers are quick to tell you that this is \textit{also} true of the software \textit{production process} itself \cite{prodprop}.

From this two things are clear:
\begin{itemize}
\item rigorous testing of any individual \textit{instruction} within a stack over it's expected domain of inputs, and a sampling of data outside of that is valuable \cite{bezier2}.
\item The interaction of an instruction with its immediate follower and predecessor must also be well understood; these multi-instruction sections can indeed be treated as though they are themselves more complicated instructions.
\end{itemize}
This is the actual theoretical underpinning behind the two testing concepts we are all familiar with (unit and integration testing), but most don't think too hard about.
It's this lack of concrete understanding that leads to frequent confusion and ineffective testsuites the world over.

It is tempting from here to consider programs as discrete stacks, which can be exhaustively proven to behave well, in a sort of "proof by induction".
However the data you feed in is itself a stack of instructions with a necessarily unbounded input and output domain.
To make things worse, well designed programs \textit{tend to} move code into data \cite{bezier3}, so now the problem not just undecidable \cite{turing} \textit{but also} recursive.

\subsection{Fitness: a further complication}

Another vexatious problem rears its head when attempting to demonstrate fitness for a particular purpose.
Any approach other than actually engaging in said purpose immediately falls afoul of the "frame problem" \cite{frame}.
Ultimately nothing but an actual user can actually accept a product is fit for a particular purpose, because non-embodied intelligence can't do that \cite{chapman}.
As such, any approach other than recording user journeys and being able to play them back is shooting in the dark.

Economists call this concept "Demonstrated Preference" and "Subjective Value"\cite{mises1},
and it is my hypothesis that the impossibility of centralized economic planning \cite{mises2} is largely the frame problem restated.
For our purposes, it is sufficient to say that while computers are quite good at dealing with cardinal concepts, they are astonishingly poor at dealing with ordinal ones.
Anyone familiar with sorting algorithms, counting problems and the "perfect covering" problem will have figured this out the hard way.
In this case, not only are we dealing with a problem which would be at least NP-Hard had we perfect knowledge, we also must grapple with incomplete information.

We are left with nothing more than the "God Algorithm" (already know the answer). \cite{atwood}
Frequently this is how caching systems are described, but to some extent this is also true of user surveillance.
It should therefore come as no surprise that tools enabling comprehensive user surveillance have proven quite popular.

Nevertheless, many firms fall victim to the conceit of planning with their "user stories" and acceptance tests based around these.
Ultimately the only \textit{effective} means to catch fitness issues early in new features is to "eat your own dog food",
being particularly attentive to "Schleps" \cite{schlep} and other places the work-flow can be de-burred or otherwise streamlined.

\section{Reasoning about Perl Programs}

Knowing all this, what then shall we do?
First, let's go over the extant tools on CPAN.

All programs can be expressed as lambda calculus; programs are nearly always transformed into some variety of "intermediate form" prior to being handed off to lower level executors.
One such form is the "abstract syntax tree", which is an intermediate form used to reason about the correctness of programs rather than to pass to hardware for execution.

Building such a tree is how PPI, and the tools which rely on it such as Perl::Critic work.
Alternatively you can inspect the "intermediate form" which the interpreter itself builds via the B::Deparse, or similar techniques as Devel::Cover and Devel::NYTProf leverage.
However just because you \textit{can} reason about perl programs in the same fashion as any other programming language, does not guarantee avenues of inquiry are exhaustively explored.

\subsection{Devel::Cover does not quite cover the subject}

Devel::Cover is quite good at identifying statements and conditions which are not covered within a program.
It cannot however identify places where a condition ought to be applied, but is not or vice versa.

It also does not have a means of identifying whether the boundaries of any given block's input/output domain have been exercised.
It is generally via these means that we discover these missed conditionals, as we are frequently surprised by the realities imposed by input data.
Given perl only has optional input signatures, and no output signatures nor ways to signify that a sub throws, this has to be inferred;
a characterization of the stack must be built.

This inference necessarily implies walking the stack down to the most basic of calls.
Suppose we have a sub foo, which calls bar() that calls baz():

\begin{lstlisting}

sub foo {
    bar(@_);
}

sub bar {
   my ($a, %b) = @_;
   baz($_[2]);
   ...
}

sub baz {
   close($_[0]) or die;
}

\end{lstlisting}

All we can say with confidence (without seeing the remaining body of bar) is that the second argument to foo() is a hash which is likely only one key/value (as randomization will break this code),
the value of which ought to be a filehandle, and which will throw in the event it isn't.

Some tool which automates this reasoning, say a Sub::Characterizer would be of great utility.
It could be used alongside Pod::Weaver annotations to largely auto-generate documentation as well.
Pod::Coverage could be similarly enhanced.

The core difficulty here is the sheer scope of such an undertaking.
A lookup table of every call in core's arguments vis perlguts datatypes, along with every FFI interaction would have to be built, likely during BEGIN.
From there, a set of boundary data for each of these datatypes would need to be maintained going forward.
Even then you'll want some means of annotating when a sub's arguments acceptable input domain is a subset of its governing datatype.
I found most of this out the "hard way" \cite{gsb} long ago attempting to solve this particular problem (and failing).

\subsection{Surely strong typing solves all this?}

Some users of other programming languages believe themselves to have obviated the entire class of errors due to improper inputs via strong typing systems.
This of course ignores the reality that poor choice of type (such as an "Any" type, or unsigned when you need signs, etc) immediately voids any such guarantees.
Boundary testing also remains necessary.
However the argument is not entirely without merit, and there are some things such as the `types` pragma and various other runtime type-checkers such as Type::Tiny to attest to this.

Typescript (a code generator) is probably the best example of a dynamic language figuring out how to give you such compile-time guarantees.
Supposing everything else it interacts with is also generated by typescript, a good degree of confidence can be had.
It's also likely that much of the code therein, particularly the types themselves, can be repurposed towards doing the same with perl.
While I wished to build a proof of concept for this, alas time did not permit.
Building a generic framework to generate dynamic language code from typed variants might be an incredibly good fit for Raku Grammars.

\subsection{I've got 100\% coverage though, its perfect right?}

It's usually fairly straightforward to get 100\% statement coverage on greenfield code.
However this rarely translates into software quality, as the test coverage on any of the dependencies in the stack is opaque in practically all programming languages.
How do we identify if the dependencies we use are well covered \textit{where we use them}?
If we can, the same techniques can be cross-referenced with the relative importance of the module vis. the "CPAN River" to identify where the most high-impact test coverage improvements could be made in the perl ecosystem.

It turns out that we can do this, supposing we actually run the testsuites of our dependencies when installing them.
This is (normally) the case when using something like perlbrew, with an isolated environment.
System perl is usually another story, as many modules will be shipped with another packaging manager.

While we could always wrap CPAN, ideally a CPAN::Plugin either pre or post test would be ideal.
Nevertheless, given the wide variety of means that CPAN Modules might be installed, any solution here would have to build a coverage index on-demand, and cache it for later use, likely as a plugin to Devel::Cover instead.
cpancover \textit{appears} to do this, but is currently broken and appears unmaintained.

Yet another place where I would love to have a proof of concept.

\subsection{eval \{\} is not enough}

Since not all programmers adhere to "Klingon Programming" (where it is better to \textit{die()} than \textit{return()} in failure),
there is no guarantee that environmental problems are caught.
This is why a great many Perl::Critic rules exist to catch precisely such problems caused by external factors.
For example:
\begin{lstlisting}
eval {...;1;} or do...

my $pid=fork(); die unless defined $pid;
\end{lstlisting}
Clearly, some means of actual fault injection should also be taken into account with regard to coverage.

How many subs are actually safe when `ls` returns `read error`?
Does your script recover from children whacked by oomkiller?
What if fork() fails because the process table is filled with zombies? \cite{gsb2}
What about when system() or qx{} or pipe open() where the target does not exist, but -f returns true because you have a bad RAID card?
I (and I suspect many of you) have had the misfortune of experiencing these cases personally.

This is actually a quite complicated subject.  As the founder of Antithesis puts it \cite{antithesis}:
\begin{displayquote}
the \$100 bill is surrounded by an entire field of poisonous spike-pits, nay, a continent of them.
\end{displayquote}
Aside from building something which can inject the \textit{astonishingly vast array} of such faults,
there is still the problem of integrating this into perl's testing harness, and reporting coverage of such in some fashion.

Given the scope, I have made no attempts at a proof of concept.
However, if I were to do so, I think the approach used by Test::MockFile \cite{mf} is likely one which can be leveraged to satisfy these needs.

\subsection{Fitness for a particular purpose}

As mentioned earlier, the best means to evaluate whether changes to software harm it's fitness to purpose is some means of tracking user paths through the software as they perform tasks.
If sudden unintended disturbances in user paths are detected after pushing changes, it is a strong indication of error.
There are two levels of this; one is front-end and the other on the back end.
Ideally these are integrated somehow; given perl is not \textit{usually} the code powering UIs, but generating them, our primary concern will be with the points that interfaces reach out and touch backend code.

The best way to satisfy these needs is via a powerful logging framework using something like Log::Dispatch::DBI and robust session handling with shared identifiers between frontend and backend instrumentation.
From there you need something to \textit{identify and classify} common patterns of usage.
This sounds suspiciously like what Neural Networks are \textit{actually good at doing}.

AI::Categorizer running on a pack()ed representation of an application's state parameters throughout a user session ought to do the trick.
Yet another place I will have to rush to get a POC if I can at all.

\subsection{Quality with non-functional Characteristics}

Devel::NYTProf is pretty much the best profiler in the industry; it's one of the primary reasons to recommend perl over alternatives.
However, the ability to reason about the accessibility, localizability or portability of your program is crude at best.
Generally the only option here is to impose linter rules which force the developer into the practices necessary to facilitate these things.
The only systematic attempt to apply linting across the perl ecosystem is CPANTS, but this could stand to be extended with more categories of evaluation.

For example, suppose I have the option of multiple modules on CPAN which I see do roughly the same thing in function X.
It would greatly simplify my decision as to what to use if I already knew ahead of time:
\begin{itemize}
\item What's the bandwidth of this function; how does it perform?
\item How complex is it?  I may have to read and debug this.
\item Is it doing non-portable things?
\end{itemize}
And so on.

This is also another place where NNs could be of great utility; while they are necessarily awful at writing \textit{correct} code \cite{copilot}, they're pretty great at classifying it.
As such we could likely build these lists of "Modules that do largely the same things on CPAN" in a mostly automated fashion.
You could roll right up to metacpan and see a "More like this" link -- like youtube but actually useful!
I for one would love seeing a table with these criteria and test coverage readily available -- no other packaging system does anything approaching this useful.

Oftentimes your choice of components is the primary determinant of your output's quality; we could do a lot more to help people reason about this.

\newpage
\section{Conclusion}

As you might expect from an active programming community of 35+ years, the things perl struggles to understand about its programs are essentially what every other programming language struggles with.
This should come as no shock, as the ultimate cause of them are pretty sticky fundamental issues of the "even people don't qualify as Strong AI" stripe.
The good news is that our near-total incapacity to percieve all the ways in which our programs can fail also means automated solutions to ferreting these out are likely to be far more effective at doing so than users.
As such the dream of "Bug Free" software is largely attainable in the same sense that a house in a bad neighborhood with cameras on it is going to be largely burgular free.
Being of relatively higher quality than all your competitors is highly desirable, as quality is the \textit{entire} driver of productivity and income.
You don't have to be original, use "alive" programming languages or even be particularly talented to win with this approach.

The software industry is one of the few places aside from SoEs where a 2/3 rate of failure \cite{standish} is remotely tolerable.
This is because our field is still relatively young (and thus high margin), but that is changing quickly.
The trend you need to get behind is not AI, but quality.
Whoever in our industry fully internalizes the lessons of Fred Brooks, Deming and Juran first will win.
Tooling such as suggested here would greatly further those ends, and give whatever language that builds them great advantage over their alternatives.

\newpage
\bibliographystyle{vancouver}
\bibliography{paper}

\end{document}
